# 🛠️ Chisel 合成数据生成器使用指南（2025.12 更新）

本文档介绍 `data_gen/generator_V2.py` 的使用姿势与最新进展。脚本用于构建高质量的 Chisel 合成数据集，服务于 LLM 的 SFT (Supervised Fine-Tuning)。

## 1. 简介

数据生成器是一个模板驱动 (Template-based) 的课程学习引擎，结合 **Curriculum Learning** 与 **Reflection Environment** 的自动验证能力。

生成流程会调用 `reflect_env` 完成编译与阐述检查，保证写入数据集的样本全部通过 "Pass@1 Compile" 校验。

## 2. 最新亮点

* **2025.12 更新 (V3)**
  * 🆕 **chisel3.util 专项训练**：新增 Level 2.5 类别 (10%)，专门训练模型掌握 `import chisel3.util._`。
  * 🆕 **8 种 util API 模板**：PopCount, Reverse, Fill, Log2, PriorityEncoder, OHToUInt, UIntToOH, Mux1H。
  * 🆕 **Cat/Enum 变体**：Level 3 新增 `shift_cat` 和 `fsm_enum` 变体，解决基线模型遗忘导入问题。
  * ✨ 指令多样性池：每个模板类型 6-10 种自然语言变体，提升 SFT 泛化能力。

* **2025.11 更新 (V2)**
  * `reflect_env.run_simulation` 全面支持 `silent` 模式，多进程日志不再污染进度条。
  * `sys.stdout.reconfigure()` 包裹在 try/except 中，兼容缺失该方法的 Python 运行时。
  * 进程池生命周期日志更加清晰，便于定位 JVM 预热阶段。

## 3. 核心特性

* **多进程加速 (Multiprocessing)**：自动检测 CPU 核心数，成倍提升生成吞吐。
* **无痕验证 (Silent Validation)**：调用 `reflect_env` 时默认静默模式，不产生多余文件。
* **课程设计 (Curriculum)**：
  * **Level 1 (45%)**：语法肌肉记忆 (Wire, Reg, Vec, Bundle)。
  * **Level 2 (30%)**：基础组合逻辑 (算术、Mux、When, Cat)。
  * **Level 2.5 util (10%)**：chisel3.util 专项 (PopCount, Reverse, Fill, Log2, etc.)。
  * **Level 3 (15%)**：时序逻辑与状态机 (Counter、ShiftReg、FSM)。
* **Chisel 6.0 兼容**：统一使用 `import chisel3._`，与当前工具链保持一致。

## 4. 环境准备

1. 激活 Conda 环境：

    ```bash
    conda activate chisel-llm
    ```

2. 确认依赖已安装（常规环境已满足，如需手动安装可执行）：

    ```bash
    pip install tqdm jinja2
    ```

> 提示：在此环境中 `python` 已映射至 Python 3.10。如宿主机默认无此别名，可直接改用 `python3` 运行下述命令。

## 5. 快速开始

### 基本用法

在仓库根目录执行脚本，命令格式：

```bash
python data_gen/generator_V2.py [目标样本数] [并发进程数]
```

#### 示例 1：生成 100 条测试数据（默认并发数）

```bash
python data_gen/generator_V2.py 100
```

#### 示例 2：生成 10,000 条数据，使用 8 个进程

```bash
python data_gen/generator_V2.py 10000 8
```

*建议并发数约为 CPU 物理核心数的一半，避免 JVM 争抢内存。*

## 6. 结果解读

运行脚本后，终端会输出类似日志：

```text
🚀 启动 Chisel 合成数据引擎 V3 (Target: 100)
⚡ 启用多进程加速: 4 workers
📊 课程分布: Level 1 (45%) | Level 2 (30%) | Level 2.5 util (10%) | Level 3 (15%)
✨ V3 新特性: chisel3.util 专项训练 | Cat/Enum/PopCount | FSM 状态机 | 错误日志
⏳ 正在初始化并行工作进程 (JVM 预热可能需要几十秒，期间进度条可能不会更新，请耐心等待)...

35%|███▌      | 35/100 [00:45<01:20,  1.24s/it, attempts=42, rate=83.3%]
```

### 关键指标说明

1. **初始化阶段**：
    * 看到 `⏳ 正在初始化...` 表示子进程正在拉起 Mill/JVM。
    * 冷启动通常需要 **30-60 秒**，进度条暂时停留在 0% 属正常现象。

2. **进度条字段**：
    * `35/100`：已写入有效样本数 / 目标样本数。
    * `1.24s/it`：生成单条有效样本的平均耗时，包含编译验证。
    * `attempts=42`：累计尝试次数（含失败项）。
    * `rate=83.3%`：当前有效率 (有效样本 / 总尝试)。若长期低于 50%，需检查模板或环境。

3. **最终结果**：
    * 数据集输出到 `dataset/chisel_sft_dataset_v2_YYYYMMDD_HHMMSS.jsonl`。
    * JSONL 中每行含 `instruction`（指令）、`input`（空字符串）、`output`（Chisel 代码）。

## 7. 进阶：扩展模板

若需覆盖更多模式，可按以下步骤扩展：

1. **定义模板**：在脚本顶部新增 Jinja2 模板字符串（例：`TEMPLATE_NEW`）。
2. **添加指令池**：在 `INSTRUCTION_TEMPLATES` 中添加对应的指令变体列表。
3. **编写生成函数**：实现 `generate_xxx` 函数，渲染模板并提供 `instruction`。
4. **注册主循环**：在 `worker_task` 中纳入新的分支逻辑，并控制权重。

## 8. 常见问题

**Q: 为什么进度条一开始不动？**  
A: JVM 预热阶段。等待各进程完成 Mill 启动即可，后续速度会明显提升。

**Q: 为什么 CPU 占用率很高？**  
A: Scala/Chisel 编译极度吃 CPU。若出现系统卡顿，请降低第二个参数以减少并发度，例如 `python data_gen/generator_V2.py 1000 4`。

**Q: 验证失败的样本会保存吗？**  
A: 不会。只有编译与阐述均通过的样本会写入 JSONL。失败样本会记录到 `logs/generation_errors_*.log`。
