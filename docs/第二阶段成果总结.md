# 第二阶段成果总结（截至 2025-12-18）

> **阶段目标：** 利用 `reflect_env`，将 LLM 生成 Chisel 代码的 **Pass@1 Compile 成功率从 <5% 提升到 >80%**。
> 
> ⚠️ **重要更新**: SFT 微调实验已完成，但结果表明该方法在当前场景下**效果不佳**，详见 [Ⅳ. SFT 微调实验总结](#ⅳ-sft-微调实验总结-)。

---

## Ⅰ. 任务一：构建 Chisel 合成数据引擎 ✅ 已完成

### 1.1 数据生成器
- **文件**: `data_gen/generator_V2.py`
- **功能**: 基于模板的批量数据生成，集成 `reflect_env` 无痕验证
- **多进程支持**: 并行验证显著提升生成效率

### 1.2 课程设计实现

| 课程等级 | 目标占比 | 实际实现 | 核心内容 |
|----------|----------|----------|----------|
| **Level 1** | 60% | ✅ | Wire/Reg/UInt/SInt/Bool 定义、基础 IO |
| **Level 2** | 30% | ✅ | Mux、When/ElseWhen、算术运算、逻辑运算、比较器 |
| **Level 3** | 10% | ✅ | 计数器、移位寄存器、边沿检测、简单 FSM |
| **Level 4** | 额外 | ✅ | 参数化模块、Valid 接口 |

### 1.3 数据资产

| 数据集 | 样本数 | 说明 |
|--------|--------|------|
| `chisel_sft_dataset_v2_20251124_081913.jsonl` | 10,000 | 主数据集 (L1-L4 基础语法) |
| `chisel_util_supplement_20251202_130105.jsonl` | 550 | 补充数据集 (chisel3.util 模块) |
| **`chisel_sft_merged_10550.jsonl`** | **10,550** | **合并后的完整训练集** ✅ |

- **数据格式**: `{"instruction": ..., "input": ..., "output": ...}`
- **验证策略**: 所有样本通过 Pass@1 Compile 验证

### 1.4 补充数据集内容 (chisel3.util)

针对 Baseline 评估中发现的 `chisel3.util` 缺失问题，补充生成了以下模块：

| 模块类型 | 样本数 | 核心 API |
|----------|--------|----------|
| FSM Enum | 100 | `Enum`, `is`, `switch` |
| PopCount | 50 | `PopCount` |
| Reverse | 50 | `Reverse` |
| Fill | 50 | `Fill` |
| Log2 | 50 | `Log2` |
| PriorityEncoder | 50 | `PriorityEncoder` |
| OHToUInt | 50 | `OHToUInt` |
| UIntToOH | 50 | `UIntToOH` |
| MuxCase | 50 | `MuxCase`, `MuxLookup` |
| ShiftCat | 50 | `Cat`, `>>`, `<<` |

### 1.5 使用方法
```bash
conda activate chisel-llm
python data_gen/generator_V2.py 100 4  # 生成100条，4进程并行
```

---

## Ⅱ. 任务二：SFT 环境搭建 ✅ 已完成

### 2.1 模型选型
- **主力模型**: Qwen2.5-Coder-14B-Instruct ✅ 已下载
- **量化方式**: 4-bit QLoRA (bitsandbytes)
- **缓存位置**: `~/.cache/huggingface/hub/`

### 2.2 环境配置
创建了**两个独立的 Conda 环境**以隔离依赖：

| 环境 | 用途 | 关键依赖 |
|------|------|---------|
| `chisel-llm` | 反射验证、数据生成 | Mill, Verilator, Python 3.10 |
| `chisel-train` | 模型训练、推理、评估 | transformers, torch, bitsandbytes, tensorboard |

### 2.3 训练工具与配置 ✅ 已完成
- **框架**: LLaMA-Factory (位于 `/home/silence_breaker/git/LLaMA-Factory`)
- **训练脚本**: `training/train_chisel_lora.sh`
- **配置文件**: `training/chisel_lora_config.yaml`

**训练参数:**
## Ⅲ. 任务三：闭环评估与 SFT 微调 ✅ 已完成

### 3.1 评估框架 ✅ 已完成
- **测试集生成**: `eval/generate_eval_set.py`
- **模型评估器**: `eval/run_eval.py`
- **测试集**: `eval/eval_set_v2.jsonl` (37 条，100% 验证通过)

### 3.2 Baseline 评估结果 ✅ 已完成

**模型**: Qwen2.5-Coder-14B-Instruct（未微调，4bit 量化）

| 指标 | 结果 | 目标 |
|------|------|------|
| **Pass@1 Compile** | **91.9% (34/37)** | >80% |

#### 按难度级别
| 级别 | 通过率 |
|------|--------|
| L1-Basic | 12/12 (100%) ✅ |
| L2-Combinational | 14/14 (100%) ✅ |
| L3-Sequential | 6/9 (67%) ⚠️ |
| L4-Advanced | 2/2 (100%) ✅ |

#### 失败分析
3 个失败用例均因**缺少 import 语句**：

| 用例 | 问题 | 缺少的 import |
|------|------|---------------|
| shift_register ×2 | `not found: value Cat` | `chisel3.util.Cat` |
| fsm_simple ×1 | `Enum is not a value` | `chisel3.util._` |

### 3.3 SFT 微调实验 ❌ 效果不佳

**训练时间**: 2025-12-02 ~ 2025-12-05
**训练时长**: ~55 小时

**训练配置:**
- 数据集: 10,550 条 (10,022 条用于训练，528 条用于验证)
- 总步数: 1,881 steps (3 epochs)
- GPU: NVIDIA RTX 4080 Super 16GB
- 最终 train_loss: 0.14
- 最终 eval_loss: 0.1392

**训练曲线表现:**
- Loss 从 0.9 快速收敛至 0.14
- 训练过程稳定，无过拟合迹象
- 但**训练指标并不能反映真实性能**

---

## Ⅳ. SFT 微调实验总结 ❌

### 4.1 微调后评估结果

| 指标 | Baseline (微调前) | 微调后 | 变化 |
|------|-------------------|--------|------|
| **Pass@1 Compile** | **91.9%** | **<50%** | 📉 **大幅下降** |

### 4.2 失败原因分析

#### 根本原因：数据集质量问题

1. **数据集制作不当**
   - 合成数据虽然通过了编译验证，但**代码风格和习惯与真实代码差异较大**
   - 模板化生成导致代码模式单一，缺乏多样性
   - 缺少真实世界中的代码上下文和注释风格

2. **灾难性遗忘 (Catastrophic Forgetting)**
   - 微调过程中，模型**丢失了原本良好的代码编写习惯**
   - 原模型对 Chisel 的泛化能力被破坏
   - 学到了数据集中的"坏习惯"，反而降低了生成质量

3. **过度拟合合成数据**
   - 模型过度适应了生成器的特定模式
   - 无法处理测试集中更多样化的真实场景

### 4.3 关键教训

> 💡 **核心结论**: 对于已经具备良好基础能力的大模型，**Prompt Engineering + Test-Time Scaling** 比 SFT 微调更具性价比。

| 方法 | 优势 | 劣势 |
|------|------|------|
| **SFT 微调** | 可针对特定任务优化 | 需要高质量数据集；可能破坏原有能力；算力消耗大 |
| **Prompt 注入** | 利用模型泛化能力；无需训练；可快速迭代 | 依赖上下文窗口；需要精心设计 prompt |
| **Test-Time Scaling** | 推理时动态增强；保留模型原有能力 | 推理成本较高 |

### 4.4 后续方向建议

基于此次失败经验，推荐以下替代方案：

1. **Prompt Engineering 优化** ⭐ 推荐
   - 设计包含 Chisel 语法规范的 System Prompt
   - 提供 few-shot 示例，特别是 `chisel3.util` 的正确用法
   - 利用 RAG 注入相关 API 文档

2. **Test-Time Scaling**
   - 多次采样 + 编译验证选择最佳
   - 自我修正（Self-Correction）机制
   - 利用 `reflect_env` 进行迭代优化

3. **如果必须 SFT**
   - 使用真实的高质量 Chisel 代码（如 rocket-chip 等开源项目）
   - 减少训练步数，避免过度拟合
   - 采用更保守的学习率
---

## 核心交付物状态

| 交付物 | 状态 | 位置 |
|--------|------|------|
| 高质量合成数据集 | ✅ 已完成 | `dataset/chisel_sft_merged_10550.jsonl` |
| 训练脚本与配置 | ✅ 已完成 | `training/` 目录 |
| 微调后的模型权重 (LoRA) | ❌ 效果不佳 | `LLaMA-Factory/outputs/chisel-coder-lora/` |
| 评估框架 | ✅ 已完成 | `eval/` 目录 |

---

## 意外发现与经验教训 📝

### 发现 1: Baseline 模型已超预期 🎉

原计划目标是将 Pass@1 Compile 从 <5% 提升到 >80%，但 Qwen2.5-Coder-14B-Instruct **未经微调**已达到 **91.9%**。

### 发现 2: SFT 微调反而有害 ⚠️

经过 55 小时的训练，微调后模型的 Pass@1 Compile 从 **91.9% 下降到 <50%**，这说明：

1. **数据质量 > 数据数量**: 低质量的合成数据会"污染"模型
2. **保护原有能力很重要**: 不当的微调会导致灾难性遗忘
3. **Test-Time Scaling 更具性价比**: 对于已经强大的基础模型，Prompt Engineering 和推理时增强比 SFT 更有效

### 结论

> 🎯 **第二阶段核心收获**: 验证了 Qwen2.5-Coder-14B 对 Chisel 的强大基础能力，并通过失败的 SFT 实验明确了**未来应聚焦于 Prompt Engineering 和 Test-Time Scaling**，而非盲目进行微调。

---

*文档更新时间: 2025-12-18*
