# 第二阶段成果总结（截至 2025-12-02）

> **阶段目标：** 利用 `reflect_env`，将 LLM 生成 Chisel 代码的 **Pass@1 Compile 成功率从 <5% 提升到 >80%**。

---

## Ⅰ. 任务一：构建 Chisel 合成数据引擎 ✅ 已完成

### 1.1 数据生成器
- **文件**: `data_gen/generator_V2.py`
- **功能**: 基于模板的批量数据生成，集成 `reflect_env` 无痕验证
- **多进程支持**: 并行验证显著提升生成效率

### 1.2 课程设计实现

| 课程等级 | 目标占比 | 实际实现 | 核心内容 |
|----------|----------|----------|----------|
| **Level 1** | 60% | ✅ | Wire/Reg/UInt/SInt/Bool 定义、基础 IO |
| **Level 2** | 30% | ✅ | Mux、When/ElseWhen、算术运算、逻辑运算、比较器 |
| **Level 3** | 10% | ✅ | 计数器、移位寄存器、边沿检测、简单 FSM |
| **Level 4** | 额外 | ✅ | 参数化模块、Valid 接口 |

### 1.3 数据资产

| 数据集 | 样本数 | 说明 |
|--------|--------|------|
| `chisel_sft_dataset_v2_20251124_081913.jsonl` | 10,000 | 主数据集 (L1-L4 基础语法) |
| `chisel_util_supplement_20251202_130105.jsonl` | 550 | 补充数据集 (chisel3.util 模块) |
| **`chisel_sft_merged_10550.jsonl`** | **10,550** | **合并后的完整训练集** ✅ |

- **数据格式**: `{"instruction": ..., "input": ..., "output": ...}`
- **验证策略**: 所有样本通过 Pass@1 Compile 验证

### 1.4 补充数据集内容 (chisel3.util)

针对 Baseline 评估中发现的 `chisel3.util` 缺失问题，补充生成了以下模块：

| 模块类型 | 样本数 | 核心 API |
|----------|--------|----------|
| FSM Enum | 100 | `Enum`, `is`, `switch` |
| PopCount | 50 | `PopCount` |
| Reverse | 50 | `Reverse` |
| Fill | 50 | `Fill` |
| Log2 | 50 | `Log2` |
| PriorityEncoder | 50 | `PriorityEncoder` |
| OHToUInt | 50 | `OHToUInt` |
| UIntToOH | 50 | `UIntToOH` |
| MuxCase | 50 | `MuxCase`, `MuxLookup` |
| ShiftCat | 50 | `Cat`, `>>`, `<<` |

### 1.5 使用方法
```bash
conda activate chisel-llm
python data_gen/generator_V2.py 100 4  # 生成100条，4进程并行
```

---

## Ⅱ. 任务二：SFT 环境搭建 ✅ 已完成

### 2.1 模型选型
- **主力模型**: Qwen2.5-Coder-14B-Instruct ✅ 已下载
- **量化方式**: 4-bit QLoRA (bitsandbytes)
- **缓存位置**: `~/.cache/huggingface/hub/`

### 2.2 环境配置
创建了**两个独立的 Conda 环境**以隔离依赖：

| 环境 | 用途 | 关键依赖 |
|------|------|---------|
| `chisel-llm` | 反射验证、数据生成 | Mill, Verilator, Python 3.10 |
| `chisel-train` | 模型训练、推理、评估 | transformers, torch, bitsandbytes, tensorboard |

### 2.3 训练工具与配置 ✅ 已完成
- **框架**: LLaMA-Factory (位于 `/home/silence_breaker/git/LLaMA-Factory`)
- **训练脚本**: `training/train_chisel_lora.sh`
- **配置文件**: `training/chisel_lora_config.yaml`

**训练参数:**
## Ⅲ. 任务三：闭环评估与 SFT 微调 🔄 进行中

### 3.1 评估框架 ✅ 已完成
- **测试集生成**: `eval/generate_eval_set.py`
- **模型评估器**: `eval/run_eval.py`
- **测试集**: `eval/eval_set_v2.jsonl` (37 条，100% 验证通过)

### 3.2 Baseline 评估结果 ✅ 已完成

**模型**: Qwen2.5-Coder-14B-Instruct（未微调，4bit 量化）

| 指标 | 结果 | 目标 |
|------|------|------|
| **Pass@1 Compile** | **91.9% (34/37)** | >80% |

#### 按难度级别
| 级别 | 通过率 |
|------|--------|
| L1-Basic | 12/12 (100%) ✅ |
| L2-Combinational | 14/14 (100%) ✅ |
| L3-Sequential | 6/9 (67%) ⚠️ |
| L4-Advanced | 2/2 (100%) ✅ |

#### 失败分析
3 个失败用例均因**缺少 import 语句**：

| 用例 | 问题 | 缺少的 import |
|------|------|---------------|
| shift_register ×2 | `not found: value Cat` | `chisel3.util.Cat` |
| fsm_simple ×1 | `Enum is not a value` | `chisel3.util._` |

### 3.3 SFT 微调 🔄 训练中

**启动时间**: 2025-12-02 15:26

**训练配置:**
- 数据集: 10,550 条 (10,022 条用于训练，528 条用于验证)
- 总步数: 1,881 steps (3 epochs)
- GPU: NVIDIA RTX 4080 Super 16GB
- 预计时长: ~50-60 小时

**训练进度** (截至 2025-12-02 18:38):
```
进度: ~5% (80/1881 steps)
Loss: 0.9 → 0.16 (快速收敛)
学习率: Cosine 衰减进行中
GPU 利用率: 100%
```

**可视化监控:**
```bash
conda activate chisel-train
tensorboard --logdir=/home/silence_breaker/git/LLaMA-Factory/outputs/chisel-coder-lora
# 浏览器访问: http://localhost:6006
```

### 3.4 待完成任务
- [x] ~~使用 LLaMA-Factory 进行 SFT 微调~~ (训练中)
- [x] ~~针对 `chisel3.util` 包加强训练数据~~ (已补充 550 条)
- [ ] 等待训练完成
- [ ] 微调后重新评估，对比提升效果
| 指标 | 结果 | 目标 |
## 核心交付物状态

| 交付物 | 状态 | 位置 |
|--------|------|------|
| 高质量合成数据集 | ✅ 已完成 | `dataset/chisel_sft_merged_10550.jsonl` |
| 训练脚本与配置 | ✅ 已完成 | `training/` 目录 |
| 微调后的模型权重 (LoRA) | 🔄 训练中 | `LLaMA-Factory/outputs/chisel-coder-lora/` |
| L1-Basic | 12/12 (100%) ✅ |
| L2-Combinational | 14/14 (100%) ✅ |
| L3-Sequential | 6/9 (67%) ⚠️ |
| L4-Advanced | 2/2 (100%) ✅ |

#### 失败分析
3 个失败用例均因**缺少 import 语句**：

这说明：
1. 该模型对 Chisel 已有较好的基础理解
2. SFT 微调的重点应放在**边缘情况**（如 `chisel3.util` 包的正确 import）
3. 已补充 550 条 `chisel3.util` 相关训练数据，预期微调后能进一步提升

---

*文档更新时间: 2025-12-02*ot a value` | `chisel3.util._` |

### 3.3 待完成任务
- [ ] 使用 LLaMA-Factory 进行 SFT 微调
- [ ] 微调后重新评估，对比提升效果
- [ ] 针对 `chisel3.util` 包加强训练数据

---

## 核心交付物状态

| 交付物 | 状态 | 位置 |
|--------|------|------|
| 高质量合成数据集 | ✅ 已完成 | `dataset/chisel_sft_dataset_v2_*.jsonl` |
| 微调后的模型权重 (LoRA) | ⏳ 待完成 | - |

---

## 意外发现 🎉

**Baseline 模型已超预期！**

原计划目标是将 Pass@1 Compile 从 <5% 提升到 >80%，但 Qwen2.5-Coder-14B-Instruct **未经微调**已达到 **91.9%**。

这说明：
1. 该模型对 Chisel 已有较好的基础理解
2. SFT 微调的重点应放在**边缘情况**（如 `chisel3.util` 包的正确 import）
3. 可考虑更严格的评估标准（如功能验证）来区分能力提升
