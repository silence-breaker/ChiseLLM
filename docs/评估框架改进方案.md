# ChiseLLM 评估框架：问题汇总与改进方案 (v1.0)

**日期:** 2025-11-25 **涉及文件:** `eval/run_eval.py`, `eval/generate_eval_set.py` **状态:** ✅ 所有问题已修复，新测试集 `eval_set_v2.jsonl` 已生成。

## 1. ✅ 已修复：代码提取鲁棒性 (Code Extraction)

### 问题描述

在 SFT 之后，部分模型（尤其是 Qwen-Coder）可能会变得“懒惰”，直接输出 Chisel 代码而不再使用 Markdown 代码块（即不包含 ```scala ...```）。 **后果：** 原有的正则匹配会失败，导致 `extract_scala_code` 返回包含解释性文字的脏数据，进而导致编译必然失败，造成评估结果偏低。

### 解决方案

在 `eval/run_eval.py` 中增加了**兜底逻辑 (Fallback Strategy)**。如果正则提取失败，但文本中包含 Chisel 关键特征（`class ... extends Module`），则认为整段文本即为代码。

**当前代码 (`eval/run_eval.py`):**

```
def extract_scala_code(text: str) -> str:
    # ... (原有的 Markdown 正则匹配) ...
    
    # [新增兜底] 挽救不写 Markdown 的模型输出
    if "class " in text and "extends Module" in text:
        return text.strip()
        
    return text # 原样返回
```

## 2. ✅ 已修复：L4 参数化模块的实例化 (Parameterized Modules)

### 问题描述

在 `generate_eval_set.py` 的 **L4-Advanced** 级别中，我们定义了参数化模块（例如 `ParamAdder`）。 `reflect_env` 的默认 Harness 是通过 `new {Module}()` 来实例化模块的（不带参数）。

- **风险场景：** 如果模型生成的代码是：`class ParamAdder(width: Int) extends Module ...` （没有默认值）。 `reflect_env` 尝试执行 `new ParamAdder()` 时，Scala 编译器会报错：`not enough arguments for constructor ParamAdder`。

### 解决方案

必须确保生成的 Chisel 代码**拥有默认参数值**，或者在 Prompt 中强制要求默认值。

**修改建议 (`eval/generate_eval_set.py` -> L4_TEMPLATES):**

1. **修改 Instruction**: 明确要求提供默认值。

2. **修改 Reference**: 确保参考代码包含默认值（你目前的代码已包含，保持即可）。

```
# L4_TEMPLATES 修改示例
{
    "category": "parameterized",
    "variants": [{"name": "Adder", "default_width": 8}],
    # [修改点] 在 Prompt 中明确要求默认值
    "instruction_template": "Write a parameterized Chisel module named `ParamAdder` taking a width parameter (default to {default_width}). ...",
    "reference_template": '''import chisel3._
class ParamAdder(width: Int = {default_width}) extends Module {{ ... }}
'''
}
```

## 3. ✅ 已修复：测试集提示词同质化 (Template Homogeneity)

### 问题描述

目前的 `generate_eval_set.py` 使用的 Prompt 句式（例如 `"Write a Chisel module named..."`）与训练集生成器 `generator_V2.py` **高度雷同**。

- **后果：** 模型可能并没有学会理解自然语言需求，而是单纯记住了“指令模板 -> 代码模板”的映射关系。这会导致评估分数**虚高**，无法真实反映模型的泛化能力。

### 改进方案

**“去套路化”**：修改测试集的 Prompt 模板，使其与训练集截然不同，模拟真实用户的提问方式。

**修改建议 (`eval/generate_eval_set.py`):**

|类型|训练集 Prompt (Current)|测试集 Prompt (Proposed)|
|---|---|---|
|**风格**|机械化、填空式|自然语言、需求描述式|
|**示例**|"Write a Chisel module named `Adder8`..."|"Implement an 8-bit adder circuit. The module name should be `Adder8`."|
|**示例**|"Define a Wire named `v_123`..."|"I need a wire signal `v_123` to connect..."|

**行动：** 建议在生成最终测试集前，手动重写 `generate_eval_set.py` 中的 `instruction_template` 字段，使其更具多样性。

## 4. 下一步行动清单 (Action Items)

> ✅ 以下所有修复项已于 2025-11-25 完成

1. **[Code] ✅ run_eval.py**: 已增强 `extract_scala_code` 函数，添加了处理不带 Markdown 代码块输出的兜底逻辑。

2. **[Data] ✅ L4 模板**: 已修改 `generate_eval_set.py` 中的 L4 模板，instruction 中明确要求 "default value"。

3. **[Data] ✅ Prompt 去套路化**: 已重写所有级别 (L1-L4) 的 instruction_template，采用自然语言描述式风格，与训练集产生差异。

4. **[Data] ✅ 新测试集**: 已生成 `eval/eval_set_v2.jsonl`，共 37 条测试用例，100% 验证通过。

### 下一步：运行基线评估

```bash
# 激活训练环境
conda activate chisel-train

# 跑基线 (Baseline) - 评估 SFT 前的 Qwen-14B 表现
python eval/run_eval.py -e eval/eval_set_v2.jsonl -m Qwen/Qwen2.5-Coder-14B-Instruct -o eval/result_baseline.json
```
