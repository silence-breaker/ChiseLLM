# ChiseLLM 评估框架使用指南

本框架用于评估 LLM 生成 Chisel 代码的能力，采用**测试集生成**与**模型评估**解耦的设计。

## 架构概览

```text
eval/
├── generate_eval_set.py   # 测试集生成器（带验证）
├── run_eval.py            # 模型评估器
├── baseline_test.py       # 旧版基线测试（已废弃）
└── eval_set_*.jsonl       # 生成的测试集
```

## 快速开始

### 1. 生成测试集（带验证）

```bash
# 激活 chisel-llm 环境
conda activate chisel-llm

# 生成并验证完整测试集 (L1-L4)
python eval/generate_eval_set.py -o eval/eval_set_v1.jsonl

# 使用多进程加速验证
python eval/generate_eval_set.py -o eval/eval_set_v1.jsonl -j 4

# 仅生成 L1-L2 基础级别
python eval/generate_eval_set.py --levels L1 L2

# 跳过验证（调试用）
python eval/generate_eval_set.py --no-verify
```

### 2. 评估模型

```bash
# 激活训练环境（需要 transformers）
conda activate chisel-train

# 评估 Qwen2.5-Coder-14B（4bit 量化）
python eval/run_eval.py -e eval/eval_set_xxx.jsonl -m Qwen/Qwen2.5-Coder-14B-Instruct

# 使用 API 服务评估
python eval/run_eval.py -e eval/eval_set_xxx.jsonl --api http://localhost:8000/v1

# 仅评估 L1 级别
python eval/run_eval.py -e eval/eval_set_xxx.jsonl --levels L1

# 快速测试（限制 5 条）
python eval/run_eval.py -e eval/eval_set_xxx.jsonl --limit 5 -v
```

### 3. 查看统计

```bash
# 分析已有结果
python eval/run_eval.py --results eval/eval_results_xxx.json --stats-only
```

## 测试集格式

每行是一个 JSON 对象：

```json
{
    "id": "L1-Basic_001",
    "level": "L1-Basic",
    "category": "passthrough",
    "instruction": "Write a Chisel module named `Passthrough8bit` with...",
    "input": "",
    "reference_code": "import chisel3._\n\nclass Passthrough8bit...",
    "test_config": {
        "require_compile": true,
        "require_elaborate": true,
        "require_simulate": false,
        "module_name": "Passthrough8bit"
    }
}
```

## 难度级别

| 级别 | 描述 | 示例 |
|------|------|------|
| L1-Basic | 基础语法 | Passthrough, Wire 定义, Reg 初始化 |
| L2-Combinational | 组合逻辑 | 加法器, MUX, 比较器, 位运算 |
| L3-Sequential | 时序逻辑 | 计数器, 移位寄存器, 边沿检测, FSM |
| L4-Advanced | 进阶特性 | 参数化模块, Valid 接口 |

## 评估指标

- **Pass@1 Compile**: 生成代码能否通过编译
- **Pass@1 Elaborate**: 生成代码能否完成阐述（生成 Verilog）
- **按级别通过率**: 各难度级别的表现
- **按类别通过率**: 各类型模块的表现
- **失败阶段分布**: 在哪个阶段失败最多

## 典型工作流

### SFT 前后对比评估

```bash
# 1. 生成固定测试集
python eval/generate_eval_set.py -o eval/eval_set_v1.jsonl --seed 42

# 2. 评估基座模型
python eval/run_eval.py -e eval/eval_set_v1.jsonl \
    -m Qwen/Qwen2.5-Coder-14B-Instruct \
    -o eval/results_baseline.json

# 3. 训练 SFT 模型...

# 4. 评估 SFT 模型
python eval/run_eval.py -e eval/eval_set_v1.jsonl \
    --api http://localhost:8000/v1 \
    -o eval/results_sft.json

# 5. 对比结果
python eval/run_eval.py --results eval/results_baseline.json --stats-only
python eval/run_eval.py --results eval/results_sft.json --stats-only
```

## 扩展测试集

编辑 `generate_eval_set.py` 中的模板数组来添加新测试用例：

```python
L2_TEMPLATES = [
    {
        "category": "new_category",
        "variants": [
            {"param1": value1, "name": "Variant1"},
        ],
        "instruction_template": "Write a module that...",
        "reference_template": '''import chisel3._
class MyModule extends Module { ... }
'''
    },
    # ...
]
```

## 注意事项

1. **环境切换**: 生成测试集用 `chisel-llm`，评估模型用 `chisel-train`
2. **显存需求**: 14B 模型 4bit 量化约需 10GB 显存
3. **评估速度**: 每条约 5-10 秒（含编译验证）
4. **API 模式**: 如果使用 LLaMA-Factory 部署，推理速度更快
