# 第二阶段 Task 2：SFT 训练环境搭建指南 (v1.0)

> **目标**: 在本地 RTX 4080 (16GB) 环境下，搭建基于 LLaMA-Factory 的高效微调环境，并完成第一次试运行。 **前置依赖**: 已完成数据生成（Task 1），持有 `chisel_sft_dataset_v2_xxxx.jsonl` 数据集。

## 1. 环境隔离策略

为了避免破坏你极其稳定的 `chisel-llm` 反射环境（特别是 Mill/Scala/Verilator 的依赖），**强烈建议**为训练任务创建一个全新的 Conda 环境。

- **`chisel-llm` (原有)**: 专门用于运行 `reflect_env.py`，负责数据生成、清洗和最终评估。

- **`chisel-train` (新增)**: 专门用于运行 PyTorch 和 LLaMA-Factory，负责模型训练。

## 2. 环境安装 (Step-by-Step)

### 2.1 创建训练环境

```
# 1. 创建新环境 (Python 3.10 是目前最稳定的选择)
conda create -n chisel-train python=3.10 -y

# 2. 激活环境
conda activate chisel-train

# 3. 安装 PyTorch (支持 CUDA 12.1)
# 这一步至关重要，必须确保 CUDA 版本正确
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```

### 2.2 安装 LLaMA-Factory

我们将使用目前最流行的微调框架 LLaMA-Factory，它完美支持 QLoRA 和 WebUI。

```
# 1. 克隆仓库 (建议放在项目根目录同级，或者作为子模块)
git clone --depth 1 [https://github.com/hiyouga/LLaMA-Factory.git](https://github.com/hiyouga/LLaMA-Factory.git)
cd LLaMA-Factory

# 2. 安装依赖 (包含 bitsandbytes, transformers, peft 等)
pip install -e ".[torch,metrics]"
```

### 2.3 验证安装

检查是否能识别到你的 4080 显卡：

```
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0)}')"
# 输出应包含: CUDA Available: True 和 NVIDIA GeForce RTX 4080
```

## 3. 数据集注册

LLaMA-Factory 需要知道你的数据在哪里。

### 3.1 准备数据

1. 找到你 Task 1 生成的最新数据集（例如 `dataset/chisel_sft_dataset_v2_20251125.jsonl`）。

2. 将其复制到 `LLaMA-Factory/data` 目录下，并重命名为 `chisel_sft.jsonl` (方便管理)。

```
# 假设你在项目根目录
cp dataset/chisel_sft_dataset_v2_*.jsonl ../LLaMA-Factory/data/chisel_sft.jsonl
```

### 3.2 修改 dataset_info.json

打开 `LLaMA-Factory/data/dataset_info.json`，在文件末尾（最后一个 `}` 之前）添加以下配置。这将告诉框架如何解析你的数据格式。

```
  "chisel_sft": {
    "file_name": "chisel_sft.jsonl",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    }
  }
```

> **注意**: 你的生成器产生的 JSONL 字段正是 `instruction`, `input`, `output`，这属于典型的 Alpaca 格式，与上述配置完美匹配。

## 4. 模型准备 (Qwen2.5-Coder-14B)

针对你的 16GB 显存，我们采用 **4-bit QLoRA** 方案训练 14B 模型。这是目前算力/效果比最高的选择。

### 4.1 自动下载 (推荐)

LLaMA-Factory 启动时会自动从 Hugging Face 下载模型。你只需要指定模型名称：`Qwen/Qwen2.5-Coder-14B-Instruct`。

> **提示**: 如果网络受限，可以使用 `huggingface-cli` 配置国内镜像或手动下载。

## 5. 启动训练 (WebUI 方式)

这是最直观的配置方式，适合第一次跑通流程。

```
# 在 LLaMA-Factory 目录下
conda activate chisel-train
llamafactory-cli webui
```

访问浏览器 `http://localhost:7860`，按以下步骤配置：

### 核心配置参数 (针对 RTX 4080 16GB)

|选项卡|参数项|推荐值|说明|
|---|---|---|---|
|**Model**|Model name|`Qwen/Qwen2.5-Coder-14B-Instruct`|基础模型|
||Quantization bit|**4**|**关键**: 必须开启 4bit 才能装进 16GB 显存|
|**Train**|Stage|Supervised Fine-Tuning|SFT 模式|
||Data dir|data||
||Dataset|`chisel_sft`|选择你刚才注册的数据集|
||**Learning Rate**|`2e-4`|QLoRA 标准学习率|
||**Epochs**|`3.0`|合成数据较干净，3轮足矣|
||**Cutoff Len**|`2048` 或 `4096`|显存允许的话尽量大，涵盖长代码|
||**Batch Size**|`2`|14B模型在16G显存下只能开很小|
||**Gradient Accumulation**|`8`|2 * 8 = 16，模拟大 Batch 效果|
||**Compute Type**|`bf16`|40系列显卡必须用 bf16|
||**Gradient Checkpointing**|**True** (开启)|**关键**: 以时间换空间，防止显存溢出|
|**LoRA**|LoRA Rank|`16`||
||LoRA Alpha|`32`||
||LoRA Dropout|`0.05`||
||Target Modules|`all`|效果最好|

### 启动步骤

1. 点击 **Preview Command** 查看生成的命令行（也可用于后续脚本化）。

2. 点击 **Start** 开始训练。

3. 观察右侧的 Loss 曲线。如果 Loss 在前 100 step 内从 2.0+ 降到 0.5 以下，说明训练正常。

## 6. 命令行启动 (脚本化方式)

如果你更喜欢命令行或需要后台挂机运行，可以创建一个启动脚本 `run_sft.sh`：

```
#!/bin/bash

# 确保在 chisel-train 环境下
# 路径根据实际情况修改
MODEL_PATH="Qwen/Qwen2.5-Coder-14B-Instruct"

llamafactory-cli train \
    --stage sft \
    --do_train \
    --model_name_or_path $MODEL_PATH \
    --dataset chisel_sft \
    --dataset_dir ./data \
    --template qwen \
    --finetuning_type lora \
    --lora_target all \
    --output_dir saves/Qwen14B_Chisel_v1 \
    --overwrite_output_dir \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 2e-4 \
    --num_train_epochs 3.0 \
    --logging_steps 10 \
    --save_steps 500 \
    --plot_loss \
    --fp16 false \
    --bf16 true \
    --quantization_bit 4 \
    --gradient_checkpointing true \
    --cutoff_len 4096
```
