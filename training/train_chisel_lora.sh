#!/bin/bash
# =============================================================================
# ChiseLLM LoRA è®­ç»ƒè„šæœ¬
# 
# ä½¿ç”¨æ–¹æ³•:
#   bash training/train_chisel_lora.sh
#
# è®­ç»ƒå¯è§†åŒ–:
#   åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ: tensorboard --logdir=outputs/chisel-coder-lora
#   ç„¶åæ‰“å¼€æµè§ˆå™¨è®¿é—®: http://localhost:6006
# =============================================================================

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# æ‰“å°å¸¦é¢œè‰²çš„æ¶ˆæ¯
print_header() {
    echo -e "${CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "${CYAN}â•‘${NC}               ğŸš€ ${GREEN}ChiseLLM SFT è®­ç»ƒå¯åŠ¨å™¨${NC}                     ${CYAN}â•‘${NC}"
    echo -e "${CYAN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
}

print_step() {
    echo -e "\n${BLUE}â–¶${NC} ${GREEN}$1${NC}"
}

print_info() {
    echo -e "  ${YELLOW}â„¹${NC} $1"
}

print_success() {
    echo -e "  ${GREEN}âœ”${NC} $1"
}

print_error() {
    echo -e "  ${RED}âœ–${NC} $1"
}

# é…ç½®è·¯å¾„
CHISEL_LLM_DIR="/home/silence_breaker/git/ChiseLLM"
LLAMA_FACTORY_DIR="/home/silence_breaker/git/LLaMA-Factory"
CONFIG_FILE="${CHISEL_LLM_DIR}/training/chisel_lora_config.yaml"
DATASET_FILE="${CHISEL_LLM_DIR}/dataset/chisel_sft_merged_10550.jsonl"
TARGET_DATASET="${LLAMA_FACTORY_DIR}/data/chisel_sft.jsonl"

# æ˜¾ç¤ºæ ‡é¢˜
print_header

# Step 1: æ£€æŸ¥ç¯å¢ƒ
print_step "æ£€æŸ¥è®­ç»ƒç¯å¢ƒ..."

# æ£€æŸ¥ conda ç¯å¢ƒ
CURRENT_ENV=$(conda info --envs | grep '*' | awk '{print $1}')
if [ "$CURRENT_ENV" != "chisel-train" ]; then
    print_error "è¯·å…ˆæ¿€æ´» chisel-train ç¯å¢ƒ: conda activate chisel-train"
    exit 1
fi
print_success "Conda ç¯å¢ƒ: chisel-train âœ“"

# æ£€æŸ¥ GPU
if ! command -v nvidia-smi &> /dev/null; then
    print_error "æœªæ£€æµ‹åˆ° NVIDIA GPU"
    exit 1
fi
GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
GPU_MEM=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader | head -1)
print_success "GPU: ${GPU_NAME} (${GPU_MEM})"

# Step 2: å‡†å¤‡æ•°æ®é›†
print_step "å‡†å¤‡æ•°æ®é›†..."

if [ ! -f "$DATASET_FILE" ]; then
    print_error "æ•°æ®é›†ä¸å­˜åœ¨: $DATASET_FILE"
    exit 1
fi

SAMPLE_COUNT=$(wc -l < "$DATASET_FILE")
print_info "æ•°æ®é›†: ${DATASET_FILE}"
print_info "æ ·æœ¬æ•°: ${SAMPLE_COUNT}"

# å¤åˆ¶æ•°æ®é›†åˆ° LLaMA-Factory
cp "$DATASET_FILE" "$TARGET_DATASET"
print_success "æ•°æ®é›†å·²å¤åˆ¶åˆ° LLaMA-Factory"

# Step 3: æ›´æ–°æ•°æ®é›†é…ç½®
print_step "é…ç½® LLaMA-Factory..."

# åˆ›å»ºç®€æ´çš„æ•°æ®é›†é…ç½®
DATASET_INFO_FILE="${LLAMA_FACTORY_DIR}/data/dataset_info.json"

# å¤‡ä»½åŸé…ç½®
if [ -f "$DATASET_INFO_FILE" ]; then
    cp "$DATASET_INFO_FILE" "${DATASET_INFO_FILE}.bak"
fi

# å†™å…¥æ–°çš„æ•°æ®é›†é…ç½®
cat > "$DATASET_INFO_FILE" << 'EOF'
{
  "chisel_sft": {
    "file_name": "chisel_sft.jsonl",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    }
  }
}
EOF

print_success "æ•°æ®é›†é…ç½®å·²æ›´æ–°"

# Step 4: æ˜¾ç¤ºè®­ç»ƒé…ç½®
print_step "è®­ç»ƒé…ç½®é¢„è§ˆ..."

echo -e "
  ${CYAN}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${NC}
  ${CYAN}â”‚${NC}  ${YELLOW}æ¨¡å‹${NC}: Qwen2.5-Coder-14B-Instruct      ${CYAN}â”‚${NC}
  ${CYAN}â”‚${NC}  ${YELLOW}æ–¹æ³•${NC}: LoRA (rank=64, alpha=128)       ${CYAN}â”‚${NC}
  ${CYAN}â”‚${NC}  ${YELLOW}æ•°æ®${NC}: ${SAMPLE_COUNT} æ ·æœ¬                      ${CYAN}â”‚${NC}
  ${CYAN}â”‚${NC}  ${YELLOW}è½®æ•°${NC}: 3 epochs                        ${CYAN}â”‚${NC}
  ${CYAN}â”‚${NC}  ${YELLOW}æ‰¹æ¬¡${NC}: 2 Ã— 8 = 16 (æœ‰æ•ˆæ‰¹æ¬¡)            ${CYAN}â”‚${NC}
  ${CYAN}â”‚${NC}  ${YELLOW}å­¦ä¹ ç‡${NC}: 2e-4 (cosine decay)           ${CYAN}â”‚${NC}
  ${CYAN}â”‚${NC}  ${YELLOW}é‡åŒ–${NC}: 4-bit (bitsandbytes)            ${CYAN}â”‚${NC}
  ${CYAN}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${NC}
"

# Step 5: å¼€å§‹è®­ç»ƒ
print_step "å¯åŠ¨ SFT è®­ç»ƒ..."
echo -e "
  ${YELLOW}ğŸ’¡ æç¤º:${NC}
  - åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹è®­ç»ƒæ›²çº¿:
    ${GREEN}tensorboard --logdir=${LLAMA_FACTORY_DIR}/outputs/chisel-coder-lora${NC}
  - ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€: ${BLUE}http://localhost:6006${NC}
  - æŒ‰ Ctrl+C å¯ä¸­æ–­è®­ç»ƒ
"

echo -e "${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${GREEN}è®­ç»ƒå¼€å§‹æ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')${NC}"
echo -e "${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"

# åˆ‡æ¢åˆ° LLaMA-Factory ç›®å½•å¹¶å¯åŠ¨è®­ç»ƒ
cd "$LLAMA_FACTORY_DIR"

# ä½¿ç”¨ llamafactory-cli å‘½ä»¤å¯åŠ¨è®­ç»ƒ
llamafactory-cli train "$CONFIG_FILE"

# è®­ç»ƒå®Œæˆ
echo -e "\n${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${GREEN}âœ… è®­ç»ƒå®Œæˆï¼${NC}"
echo -e "${GREEN}è®­ç»ƒç»“æŸæ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')${NC}"
echo -e "${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"

print_info "æ¨¡å‹ä¿å­˜ä½ç½®: ${LLAMA_FACTORY_DIR}/outputs/chisel-coder-lora"
print_info "ä¸‹ä¸€æ­¥: è¿è¡Œè¯„ä¼°è„šæœ¬æµ‹è¯•æ¨¡å‹æ•ˆæœ"
echo -e "  ${GREEN}python eval/run_eval.py --model ${LLAMA_FACTORY_DIR}/outputs/chisel-coder-lora${NC}"
