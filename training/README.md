# ğŸš€ ChiseLLM æ¨¡å‹å¾®è°ƒæŒ‡å—

## ğŸ“Š è®­ç»ƒå¯è§†åŒ–æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ChiseLLM SFT è®­ç»ƒæµç¨‹                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   ğŸ“ æ•°æ®é›† (10,550 æ ·æœ¬)                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚   â”‚ chisel_sft_merged   â”‚                                                   â”‚
â”‚   â”‚ â”œâ”€â”€ L1: åŸºç¡€è¯­æ³•     â”‚                                                   â”‚
â”‚   â”‚ â”œâ”€â”€ L2: ç»„åˆé€»è¾‘     â”‚                                                   â”‚
â”‚   â”‚ â”œâ”€â”€ L3: æ—¶åºé€»è¾‘     â”‚                                                   â”‚
â”‚   â”‚ â”œâ”€â”€ L4: å‚æ•°åŒ–æ¨¡å—   â”‚                                                   â”‚
â”‚   â”‚ â””â”€â”€ Util: chisel3.util â”‚                                                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚              â”‚                                                              â”‚
â”‚              â–¼                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚   â”‚   æ•°æ®é¢„å¤„ç†         â”‚      â”‚   æ¨¡å‹åŠ è½½ (4-bit)   â”‚                     â”‚
â”‚   â”‚   - Tokenization    â”‚      â”‚   - Qwen2.5-Coder   â”‚                     â”‚
â”‚   â”‚   - Padding         â”‚ â”€â”€â”€â”€ â”‚   - 14B å‚æ•°        â”‚                     â”‚
â”‚   â”‚   - Batching        â”‚      â”‚   - LoRA é€‚é…å™¨      â”‚                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              â”‚                            â”‚                                 â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                           â–¼                                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚              â”‚      SFT è®­ç»ƒ       â”‚                                        â”‚
â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                                        â”‚
â”‚              â”‚  â”‚ Epoch 1/3     â”‚  â”‚                                        â”‚
â”‚              â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 40% â”‚  â”‚                                        â”‚
â”‚              â”‚  â”‚ Loss: 0.532   â”‚  â”‚                                        â”‚
â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                           â”‚                                                 â”‚
â”‚                           â–¼                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚   â”‚           ğŸ“ˆ å¯è§†åŒ–ç›‘æ§ (TensorBoard)      â”‚                             â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                             â”‚
â”‚   â”‚   â”‚  Loss  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â”‚                             â”‚
â”‚   â”‚   â”‚  LR    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â”‚                             â”‚
â”‚   â”‚   â”‚  GPU   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â”‚                             â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                           â”‚                                                 â”‚
â”‚                           â–¼                                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚              â”‚   ğŸ’¾ ä¿å­˜ LoRA æ¨¡å‹  â”‚                                        â”‚
â”‚              â”‚   outputs/chisel-   â”‚                                        â”‚
â”‚              â”‚   coder-lora        â”‚                                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ å‰ç½®å‡†å¤‡

### 1. ç¯å¢ƒè¦æ±‚
- **GPU**: NVIDIA GPU with >= 24GB VRAM (æ¨è RTX 3090/4090 æˆ– A100)
- **CUDA**: 11.8 æˆ– 12.1
- **Python**: 3.10+
- **Conda ç¯å¢ƒ**: `chisel-train`

### 2. æ£€æŸ¥ GPU
```bash
nvidia-smi
```

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### Step 1: æ¿€æ´»è®­ç»ƒç¯å¢ƒ
```bash
conda activate chisel-train
```

### Step 2: é…ç½®æ•°æ®é›†
```bash
# å¤åˆ¶æ•°æ®é›†åˆ° LLaMA-Factory
cp /home/silence_breaker/git/ChiseLLM/dataset/chisel_sft_merged_10550.jsonl \
   /home/silence_breaker/git/LLaMA-Factory/data/chisel_sft.jsonl

# æ›´æ–°æ•°æ®é›†é…ç½® (å·²è‡ªåŠ¨é…ç½®)
```

### Step 3: å¯åŠ¨è®­ç»ƒ
```bash
cd /home/silence_breaker/git/LLaMA-Factory

# æ–¹å¼ä¸€: ä½¿ç”¨å‘½ä»¤è¡Œ (æ¨è)
bash /home/silence_breaker/git/ChiseLLM/training/train_chisel_lora.sh

# æ–¹å¼äºŒ: ä½¿ç”¨ Web UI
python -m llamafactory.webui.interface
```

### Step 4: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
```bash
# åœ¨æ–°ç»ˆç«¯çª—å£è¿è¡Œ
tensorboard --logdir=/home/silence_breaker/git/LLaMA-Factory/outputs/chisel-coder-lora
```
ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€: http://localhost:6006

## ğŸ“Š è®­ç»ƒå‚æ•°è¯´æ˜

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `model_name` | Qwen2.5-Coder-14B-Instruct | åŸºåº§æ¨¡å‹ |
| `num_train_epochs` | 3 | è®­ç»ƒè½®æ•° |
| `per_device_batch_size` | 2 | æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å° |
| `gradient_accumulation` | 8 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `learning_rate` | 2e-4 | å­¦ä¹ ç‡ |
| `lora_rank` | 64 | LoRA ç§© |
| `lora_alpha` | 128 | LoRA ç¼©æ”¾å› å­ |
| `quantization` | 4-bit | é‡åŒ–ä½æ•° (èŠ‚çœæ˜¾å­˜) |

## ğŸ“ˆ é¢„æœŸè®­ç»ƒæ›²çº¿

```
Loss
  â”‚
3.0â”œâ”€â”€â—
   â”‚    â•²
2.0â”œâ”€â”€â”€â”€â”€â—â”€â”€â•²
   â”‚          â•²
1.0â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â—
   â”‚
0.0â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â–¶ Steps
        0   500  1000  1500  2000
```

- **åˆæœŸ (0-500 steps)**: Loss å¿«é€Ÿä¸‹é™ï¼Œæ¨¡å‹å­¦ä¹ åŸºæœ¬è¯­æ³•
- **ä¸­æœŸ (500-1500 steps)**: Loss å¹³ç¨³ä¸‹é™ï¼Œå­¦ä¹ å¤æ‚æ¨¡å¼
- **åæœŸ (1500+ steps)**: Loss è¶‹äºç¨³å®šï¼Œå¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆ

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: CUDA Out of Memory
```bash
# å‡å° batch size
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
```

### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢
```bash
# å¯ç”¨ flash attention
flash_attn: fa2

# æˆ–ä½¿ç”¨ bf16 ç²¾åº¦
bf16: true
```

### Q3: å¦‚ä½•æ¢å¤è®­ç»ƒ
```bash
# åœ¨é…ç½®ä¸­æŒ‡å®š checkpoint
resume_from_checkpoint: outputs/chisel-coder-lora/checkpoint-500
```

## ğŸ¯ è®­ç»ƒå®Œæˆå

### 1. åˆå¹¶ LoRA æƒé‡ (å¯é€‰)
```bash
python -m llamafactory.cli.export \
    --model_name_or_path Qwen/Qwen2.5-Coder-14B-Instruct \
    --adapter_name_or_path outputs/chisel-coder-lora \
    --export_dir outputs/chisel-coder-merged \
    --export_size 2
```

### 2. è¯„ä¼°æ¨¡å‹
```bash
cd /home/silence_breaker/git/ChiseLLM
conda activate chisel-train
python eval/run_eval.py \
    --model outputs/chisel-coder-lora \
    --eval-set eval/eval_set_v1.jsonl
```

## ğŸ“ æ–‡ä»¶ç»“æ„
```
ChiseLLM/training/
â”œâ”€â”€ README.md                    # æœ¬æ–‡æ¡£
â”œâ”€â”€ train_chisel_lora.sh         # è®­ç»ƒå¯åŠ¨è„šæœ¬
â”œâ”€â”€ chisel_lora_config.yaml      # è®­ç»ƒé…ç½®æ–‡ä»¶
â””â”€â”€ setup_dataset.sh             # æ•°æ®é›†é…ç½®è„šæœ¬
```
